Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Khader2020a,
abstract = {Reinforcement learning (RL) has had its fair share of success in contact-rich manipulation tasks but it still lags behind in benefiting from advances in robot control theory such as impedance control and stability guarantees. Recently, the concept of variable impedance control (VIC) was adopted into RL with encouraging results. However, the more important issue of stability remains unaddressed. To clarify the challenge in stable RL, we introduce the term all-the-time-stability that unambiguously means that every possible rollout will be stability certified. Our contribution is a model-free RL method that not only adopts VIC but also achieves all-the-time-stability. Building on a recently proposed stable VIC controller as the policy parameterization, we introduce a novel policy search algorithm that is inspired by Cross-Entropy Method and inherently guarantees stability. As a part of our extensive experimental studies, we report, to the best of our knowledge, the first successful application of RL with all-the-time-stability on the benchmark problem of peg-in-hole.},
archivePrefix = {arXiv},
arxivId = {2004.10886},
author = {Khader, Shahbaz A. and Yin, Hang and Falco, Pietro and Kragic, Danica},
eprint = {2004.10886},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Khader et al. - 2020 - Stability-Guaranteed Reinforcement Learning for Contact-rich Manipulation.pdf:pdf},
title = {{Stability-Guaranteed Reinforcement Learning for Contact-rich Manipulation}},
url = {http://arxiv.org/abs/2004.10886},
year = {2020}
}
@article{Fujimoto2018d,
abstract = {In value-based reinforcement learning methods such as deep Q-leaming, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit over- estimation. We draw the conncction between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
annote = {TD3 Original Paper},
archivePrefix = {arXiv},
arxivId = {1802.09477},
author = {Fujimoto, Scott and {Van Hoof}, Herke and Meger, David},
eprint = {1802.09477},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/1802.09477.pdf:pdf},
isbn = {9781510867963},
issn = {1938-7228},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {2587--2601},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
volume = {4},
year = {2018}
}
@article{Gay2007a,
annote = {Q-Learning Origial Paper},
author = {Gay, Gregory R. and Salomoni, Paola and Mirri, Silvia},
doi = {10.4018/978-1-59140-993-9.ch026},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Gay, Salomoni, Mirri - 2007 - Q-Learning.pdf:pdf},
isbn = {9781591409939},
journal = {Encyclopedia of Internet Technologies and Applications},
keywords = {-learning,0,asynchronous dynamic programming,reinforcement learning,temporal differences},
pages = {179--184},
title = {{Q-Learning}},
volume = {292},
year = {2007}
}
@article{Qudsi,
author = {Qudsi, Yasmeen and Vaughan, Joshua},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Qudsi, Vaughan - Unknown - Flexibility in Insectoid Robots Design and Control of Compliant Lower Legs Tibia(2).pdf:pdf},
keywords = {biomimicry,command generation,compliance,input shaping,insectoid robot},
title = {{Flexibility in Insectoid Robots Design and Control of Compliant Lower Legs Tibia}}
}
@article{Pradhan2012e,
abstract = {This paper exploits reinforcement learning (RL) for developing real-time adaptive control of tip trajectory and deflection of a two-link flexible manipulator handling variable payloads. This proposed adaptive controller consists of a proportional derivative (PD) tracking loop and an actor-critic-based RL loop that adapts the actor and critic weights in response to payload variations while suppressing the tip deflection and tracking the desired trajectory. The actor-critic-based RL loop uses a recursive least square (RLS)-based temporal difference (TD) learning with eligibility trace and an adaptive memory to estimate the critic weights and a gradient-based estimator for estimating actor weights. Tip trajectory tracking and suppression of tip deflection performances of the proposed RL-based adaptive controller (RLAC) are compared with that of a nonlinear regression-based direct adaptive controller (DAC) and a fuzzy learning-based adaptive controller (FLAC). Simulation and experimental results envisage that the RLAC outperforms both the DAC and FLAC. {\textcopyright} 2012 IEEE.},
author = {Pradhan, Santanu Kumar and Subudhi, Bidyadhar},
doi = {10.1109/TASE.2012.2189004},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Pradhan, Subudhi - 2012 - Real-time adaptive control of a flexible manipulator using reinforcement learning(2).pdf:pdf},
issn = {15455955},
journal = {IEEE Transactions on Automation Science and Engineering},
keywords = {Adaptive control,flexible-link manipulator,reinforcement learning,tip trajectory tracking},
number = {2},
pages = {237--249},
publisher = {IEEE},
title = {{Real-time adaptive control of a flexible manipulator using reinforcement learning}},
volume = {9},
year = {2012}
}
@article{Mniha,
annote = {DQN Paper},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5602v1},
author = {Mnih, Volodymyr and Silver, David},
eprint = {arXiv:1312.5602v1},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Mnih, Silver - Unknown - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}}
}
@article{Vaughan2013,
abstract = {Under certain conditions, hopping, skipping, or jumping might be a more efficient means of terrestrial locomotion than either walking or rolling. In addition, for many robotic applications, a robust jumping algorithm could be easier to implement than walking. Jumping also provides a means to cross obstacles that might otherwise stop a wheeled or walking robot. This paper will examine a special case of jumping and generation of jumping commands. A simplified flexible robotic leg is modeled and jumping commands are generated utilizing a command shaping algorithm.},
annote = {cite for control input},
author = {Vaughan, Joshua},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/06695635.pdf:pdf},
isbn = {7777777777},
publisher = {IEEE},
title = {{Jumping Commands For Flexible-Legged Robots}},
year = {2013}
}
@article{Ha2019j,
abstract = {In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agentʼs physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agentʼs design that is better suited for its task, jointly with the policy. We propose an alteration to the popular OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications.},
annote = {From Vaughan},
archivePrefix = {arXiv},
arxivId = {1810.03779},
author = {Ha, David},
doi = {10.1162/artl_a_00301},
eprint = {1810.03779},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Ha - 2019 - Reinforcement learning for improving agent design(2).pdf:pdf},
issn = {15309185},
journal = {Artificial Life},
keywords = {Deep reinforcement learning,Evolution strategies,Generative design,Neuroevolution},
number = {4},
pages = {352--365},
pmid = {31697584},
title = {{Reinforcement learning for improving agent design}},
volume = {25},
year = {2019}
}
@article{Schulman2017b,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
annote = {PPO Original Paper},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Pagliuca2020,
abstract = {We analyze the efficacy of modern neuro-evolutionary strategies for continuous control optimization. Overall, the results collected on a wide variety of qualitatively different benchmark problems indicate that these methods are generally effective and scale well with respect to the number of parameters and the complexity of the problem. Moreover, they are relatively robust with respect to the setting of hyper-parameters. The comparison of the most promising methods indicates that the OpenAI-ES algorithm outperforms or equals the other algorithms on all considered problems. Moreover, we demonstrate how the reward functions optimized for reinforcement learning methods are not necessarily effective for evolutionary strategies and vice versa. This finding can lead to reconsideration of the relative efficacy of the two classes of algorithm since it implies that the comparisons performed to date are biased toward one or the other class.},
archivePrefix = {arXiv},
arxivId = {1912.05239},
author = {Pagliuca, Paolo and Milano, Nicola and Nolfi, Stefano},
doi = {10.3389/frobt.2020.00098},
eprint = {1912.05239},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/frobt-07-00098.pdf:pdf},
issn = {22969144},
journal = {Frontiers in Robotics and AI},
keywords = {continuous control optimization,evolutionary strategies,fitness function design,natural evolutionary strategies,reinforcement learning},
number = {July},
pages = {1--13},
title = {{Efficacy of Modern Neuro-Evolutionary Strategies for Continuous Control Optimization}},
volume = {7},
year = {2020}
}
@article{Chen2020,
abstract = {Deep Reinforcement Learning (RL) has shown great success in learning complex control policies for a variety of applications in robotics. However, in most such cases, the hardware of the robot has been considered immutable, modeled as part of the environment. In this study, we explore the problem of learning hardware and control parameters together in a unified RL framework. To achieve this, we propose to model aspects of the robot's hardware as a “mechanical policy”, analogous to and optimized jointly with its computational counterpart. We show that, by modeling such mechanical policies as auto-differentiable computational graphs, the ensuing optimization problem can be solved efficiently by gradient-based algorithms from the Policy Optimization family. We present two such design examples: a toy mass-spring problem, and a real-world problem of designing an underactuated hand. We compare our method against traditional co-optimization approaches, and also demonstrate its effectiveness by building a physical prototype based on the learned hardware parameters.},
archivePrefix = {arXiv},
arxivId = {2008.04460},
author = {Chen, Tianjian and He, Zhanpeng and Ciocarlie, Matei},
eprint = {2008.04460},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/2008.04460.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Mechanical-Computational Co-Optimization,Reinforcement Learning},
number = {CoRL},
title = {{Hardware as Policy: Mechanical and computational co-optimization using deep reinforcement learning}},
year = {2020}
}
@article{Horigomed,
author = {Horigome, Atsushi and Qudsi, Yasmeen and Fisher, Erin and Vaughan, Joshua},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Horigome et al. - Unknown - Robot Jumping with Curved-Beam Flexible Legs Orange marker Blue marker Tether(2).pdf:pdf},
title = {{Robot Jumping with Curved-Beam Flexible Legs Orange marker Blue marker Tether}}
}
@article{Luck2019,
abstract = {Humans and animals are capable of quickly learning new behaviours to solve new tasks. Yet, we often forget that they also rely on a highly specialized morphology that co-adapted with motor control throughout thousands of years. Although compelling, the idea of co-adapting morphology and behaviours in robots is often unfeasible because of the long manufacturing times, and the need to redesign an appropriate controller for each morphology. In this paper, we propose a novel approach to automatically and efficiently co-adapt a robot morphology and its controller. Our approach is based on recent advances in deep reinforcement learning, and specifically the soft actor critic algorithm. Key to our approach is the possibility of leveraging previously tested morphologies and behaviors to estimate the performance of new candidate morphologies. As such, we can make full use of the information available for making more informed decisions, with the ultimate goal of achieving a more data-efficient co-adaptation (i.e., reducing the number of morphologies and behaviors tested). Simulated experiments show that our approach requires drastically less design prototypes to find good morphology-behaviour combinations, making this method particularly suitable for future co-adaptation of robot designs in the real world.},
annote = {Learning design parameters},
archivePrefix = {arXiv},
arxivId = {1911.06832},
author = {Luck, Kevin Sebastian and Amor, Heni Ben and Calandra, Roberto},
eprint = {1911.06832},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/1911.06832.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Co-adaptation,Deep Reinforcement Learning,Morphology},
number = {CoRL},
title = {{Data-efficient co-adaptation of morphology and behaviour with deep reinforcement learning}},
year = {2019}
}
@article{Ouyang2017d,
abstract = {In this study, the authors focus on the reinforcement learning control of a single-link flexible manipulator and attempt to suppress the vibration due to its flexibility and lightweight structure. The assumed mode method and the Lagrange's equation are adopted in modelling to enhance the satisfaction of precision. Two radial basis function neural networks (NNs) are employed in the designed control algorithm, actor NN for generating a policy and critic NN for evaluating the cost-to-go. Rigorous stability of the system has been proven via Lyapunov's direct method. Through Matlab simulation and experiment on the Quanser flexible link platform, the superiority and feasibility of the reinforcement learning control are verified.},
author = {Ouyang, Yuncheng and He, Wei and Li, Xiajing},
doi = {10.1049/iet-cta.2016.1540},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Ouyang, He, Li - 2017 - Reinforcement learning control of a singlelink flexible robotic manipulator(3).pdf:pdf},
issn = {17518652},
journal = {IET Control Theory and Applications},
number = {9},
pages = {1426--1433},
title = {{Reinforcement learning control of a singlelink flexible robotic manipulator}},
volume = {11},
year = {2017}
}
@article{Gupta2021a,
abstract = {The intertwined processes of learning and evolution in complex environmental niches have resulted in a remarkable diversity of morphological forms. Moreover, many aspects of animal intelligence are deeply embodied in these evolved morphologies. However, the principles governing relations between environmental complexity, evolved morphology, and the learnability of intelligent control, remain elusive, partially due to the substantial challenge of performing large-scale in silico experiments on evolution and learning. We introduce Deep Evolutionary Reinforcement Learning (DERL): a novel computational framework which can evolve diverse agent morphologies to learn challenging locomotion and manipulation tasks in complex environments using only low level egocentric sensory information. Leveraging DERL we demonstrate several relations between environmental complexity, morphological intelligence and the learnability of control. First, environmental complexity fosters the evolution of morphological intelligence as quantified by the ability of a morphology to facilitate the learning of novel tasks. Second, evolution rapidly selects morphologies that learn faster, thereby enabling behaviors learned late in the lifetime of early ancestors to be expressed early in the lifetime of their descendants. In agents that learn and evolve in complex environments, this result constitutes the first demonstration of a long-conjectured morphological Baldwin effect. Third, our experiments suggest a mechanistic basis for both the Baldwin effect and the emergence of morphological intelligence through the evolution of morphologies that are more physically stable and energy efficient, and can therefore facilitate learning and control.},
annote = {from vaughan},
archivePrefix = {arXiv},
arxivId = {2102.02202},
author = {Gupta, Agrim and Savarese, Silvio and Ganguli, Surya and Fei-Fei, Li},
eprint = {2102.02202},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Gupta et al. - 2021 - Embodied Intelligence via Learning and Evolution.pdf:pdf},
pages = {1--18},
title = {{Embodied Intelligence via Learning and Evolution}},
url = {http://arxiv.org/abs/2102.02202},
volume = {4},
year = {2021}
}
@article{Bhagat2019e,
abstract = {The increasing trend of studying the innate softness of robotic structures and amalgamating it with the benefits of the extensive developments in the field of embodied intelligence has led to the sprouting of a relatively new yet rewarding sphere of technology in intelligent soft robotics. The fusion of deep reinforcement algorithms with soft bio-inspired structures positively directs to a fruitful prospect of designing completely self-sufficient agents that are capable of learning from observations collected from their environment. For soft robotic structures possessing countless degrees of freedom, it is at times not convenient to formulate mathematical models necessary for training a deep reinforcement learning (DRL) agent. Deploying current imitation learning algorithms on soft robotic systems has provided competent results. This review article posits an overview of various such algorithms along with instances of being applied to real-world scenarios, yielding frontier results. Brief descriptions highlight the various pristine branches of DRL research in soft robotics.},
author = {Bhagat, Sarthak and Banerjee, Hritwick and Tse, Zion Tsz Ho and Ren, Hongliang},
doi = {10.3390/robotics8010004},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Bhagat et al. - 2019 - Deep reinforcement learning for soft, flexible robots Brief review with impending challenges(3).pdf:pdf},
issn = {22186581},
journal = {Robotics},
keywords = {Deep reinforcement learning,Imitation learning,Soft robotics},
number = {1},
pages = {1--36},
title = {{Deep reinforcement learning for soft, flexible robots: Brief review with impending challenges}},
volume = {8},
year = {2019}
}
@article{Salimans2017g,
abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
annote = {From Vaughan},
archivePrefix = {arXiv},
arxivId = {1703.03864},
author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
eprint = {1703.03864},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Salimans et al. - 2017 - Evolution strategies as a scalable alternative to reinforcement learning.pdf:pdf},
journal = {arXiv},
pages = {1--13},
title = {{Evolution strategies as a scalable alternative to reinforcement learning}},
year = {2017}
}
@article{Dwiel2019d,
abstract = {The use of robotics in controlled environments has flourished over the last several decades and training robots to perform tasks using control strategies developed from dynamical models of their hardware have proven very effective. However, in many real-world settings, the uncertainties of the environment, the safety requirements and generalized capabilities that are expected of robots make rigid industrial robots unsuitable. This created great research interest in developing control strategies for flexible robot hardware for which building dynamical models are challenging. In this paper, inspired by the success of deep reinforcement learning (DRL), we systematically study the efficacy of policy search methods using DRL in training flexible robots. Our results indicate that DRL is successfully able to learn efficient and robust policies for complex tasks at various degrees of flexibility. We also note that DRL using Deep Deterministic Policy Gradients can be sensitive to the choice of sensors and adding more informative sensors does not necessarily make the task easier to learn.},
archivePrefix = {arXiv},
arxivId = {1907.00269},
author = {Dwiel, Zach and Candadai, Madhavun and Phielipp, Mariano},
doi = {10.1109/IROS40897.2019.8968516},
eprint = {1907.00269},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Dwiel, Candadai, Phielipp - 2019 - On Training Flexible Robots using Deep Reinforcement Learning(2).pdf:pdf},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4666--4671},
title = {{On Training Flexible Robots using Deep Reinforcement Learning}},
year = {2019}
}
@article{Buchli2011b,
abstract = {One of the hallmarks of the performance, versatility, and robustness of biological motor control is the ability to adapt the impedance of the overall biomechanical system to different task requirements and stochastic disturbances. A transfer of this principle to robotics is desirable, for instance to enable robots to work robustly and safely in everyday human environments. It is, however, not trivial to derive variable impedance controllers for practical high degree-of-freedom (DOF) robotic tasks. In this contribution, we accomplish such variable impedance control with the reinforcement learning (RL) algorithm PI2 (Policy Improvement with Path Integrals). PI2 is a model-free, sampling-based learning method derived from first principles of stochastic optimal control. The PI2 algorithm requires no tuning of algorithmic parameters besides the exploration noise. The designer can thus fully focus on the cost function design to specify the task. From the viewpoint of robotics, a particular useful property of PI2 is that it can scale to problems of many DOFs, so that reinforcement learning on real robotic systems becomes feasible. We sketch the PI2 algorithm and its theoretical properties, and how it is applied to gain scheduling for variable impedance control. We evaluate our approach by presenting results on several simulated and real robots. We consider tasks involving accurate tracking through via points, and manipulation tasks requiring physical contact with the environment. In these tasks, the optimal strategy requires both tuning of a reference trajectory and the impedance of the end-effector. The results show that we can use path integral based reinforcement learning not only for planning but also to derive variable gain feedback controllers in realistic scenarios. Thus, the power of variable impedance control is made available to a wide variety of robotic systems and practical applications. {\textcopyright} The Author(s) 2011.},
author = {Buchli, Jonas and Stulp, Freek and Theodorou, Evangelos and Schaal, Stefan},
doi = {10.1177/0278364911402527},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Buchli et al. - 2011 - Learning variable impedance control.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Compliant control,Gain scheduling,Motion primitives,Reinforcement learning,Stochastic optimal control,Variable impedance control},
number = {7},
pages = {820--833},
title = {{Learning variable impedance control}},
volume = {30},
year = {2011}
}
@article{Silver2014a,
abstract = {2014 In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
annote = {Polciy Gradient Paper},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Silver et al. - 2014 - Deterministic policy gradient algorithms.pdf:pdf},
isbn = {9781634393973},
journal = {31st International Conference on Machine Learning, ICML 2014},
pages = {605--619},
title = {{Deterministic policy gradient algorithms}},
volume = {1},
year = {2014}
}
@article{Schaff2019e,
abstract = {The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learning-based approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical - i.e., by picking a design and training a control policy for it. Since training these policies is time-consuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that jointly optimizes over the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines across different settings.},
archivePrefix = {arXiv},
arxivId = {1801.01432},
author = {Schaff, Charles and Yunis, David and Chakrabarti, Ayan and Walter, Matthew R.},
doi = {10.1109/ICRA.2019.8793537},
eprint = {1801.01432},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Schaff et al. - 2019 - Jointly learning to construct and control agents using deep reinforcement learning.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {9798--9805},
title = {{Jointly learning to construct and control agents using deep reinforcement learning}},
volume = {2019-May},
year = {2019}
}
@article{Lillicrap2016h,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
annote = {DDPG Original Paper},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Lillicrap et al. - 2016 - Continuous control with deep reinforcement learning.pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
title = {{Continuous control with deep reinforcement learning}},
year = {2016}
}
@article{Cui2019e,
abstract = {Most researches about control of flexible manipulators are all based on the dynamic model, which is difficult to establish because of their flexibility and the tedious process of measuring flexible link's parameters. In this paper, the goal is to design a controller which is able to control the flexible manipulator to track a given position in joint space and suppress vibration without knowing the dynamic model. For the problem of tracking a given position, a tracking controller is designed based on sliding mode control, and for the purpose of vibration suppression, a vibration suppression controller is designed as a deep neural network. Because the input of the flexible manipulator, torques at each joint, is a high dimensional and continuous space, Deep Deterministic Policy Gradient Algorithm (DDPG) is adopted to train the neural network in the vibration suppression controller. The effectiveness of the proposed controller to track a given position and suppress vibration is demonstrated by numerical simulation.},
author = {Cui, Leilei and Chen, Weidong and Wang, Hesheng and Wang, Jingchuan},
doi = {10.1109/CAC.2018.8623788},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Cui et al. - 2019 - Control of Flexible Manipulator Based on Reinforcement Learning(2).pdf:pdf},
journal = {Proceedings 2018 Chinese Automation Congress, CAC 2018},
keywords = {flexible manipulator,reinforcement learning,vibration suppression},
pages = {2744--2749},
publisher = {IEEE},
title = {{Control of Flexible Manipulator Based on Reinforcement Learning}},
year = {2019}
}
@article{Hu2020,
abstract = {We consider the problem of optimizing a robot morphology to achieve the best performance for a target task, under computational resource limitations. The evaluation process for each morphological design involves learning a controller for the design, which can consume substantial time and computational resources. To address the challenge of expensive robot morphology evaluation, we present a continuous multi-fidelity Bayesian Optimization framework that efficiently utilizes computational resources via low-fidelity evaluations. We identify the problem of non-stationarity over fidelity space. Our proposed fidelity warping mechanism can learn representations of learning epochs and tasks to model non-stationary covariances between continuous fidelity evaluations which prove challenging for off-the-shelf stationary kernels. Various experiments demonstrate that our method can utilize the low-fidelity evaluations to efficiently search for the optimal robot morphology, outperforming state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {2012.04195},
author = {Hu, Sha and Yang, Zeshi and Mori, Greg},
eprint = {2012.04195},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/2012.04195.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Neural fidelity warping for efficient robot morphology design}},
year = {2020}
}
@article{He2020f,
author = {He, Wei and Gao, Hejia and Zhou, Chen and Yang, Chenguang and Li, Zhijun},
doi = {10.1109/tsmc.2020.2975232},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/He et al. - 2020 - Reinforcement Learning Control of a Flexible Two-Link Manipulator An Experimental Investigation(2).pdf:pdf},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
pages = {1--11},
title = {{Reinforcement Learning Control of a Flexible Two-Link Manipulator: An Experimental Investigation}},
year = {2020}
}
