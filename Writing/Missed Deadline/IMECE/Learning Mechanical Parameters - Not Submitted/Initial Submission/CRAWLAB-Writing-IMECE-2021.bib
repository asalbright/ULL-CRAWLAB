Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop
@article{Sorensen:08d,
	abstract = {Input shaping is a well-established technique used for reducing the vibratory response of dynamic systems. Analytical tools are available for systems utilizing input shaping. These tools aid in performance analysis by providing intuitive and computationally simple methods for determining key system attributes, such as the residual vibration in response to a command. This paper describes methods whereby arbitrary reference commands may be interpreted as input-shaped commands. This capability allows input shaping analysis tools to be used on systems without input shapers. Experimental results obtained from an industrial 10-ton bridge crane validate the theoretical developments. &copy; 2008 Elsevier Ltd. All rights reserved.},
	address = {Langford Lane, Kidlington, Oxford, OX5 1GB, United Kingdom},
	author = {Sorensen, Khalid L. and Singhose, William E.},
	copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
	date-added = {2021-04-29 04:09:23 -0400},
	date-modified = {2021-04-29 04:09:23 -0400},
	issn = {00051098},
	journal = {Automatica},
	key = {Dynamic response},
	keywords = {Lattice vibrations;Vibrations (mechanical);},
	language = {English},
	note = {Analysis tools;Analytical tools;Command shaping;Deconvolution;Dynamic Systems;Induced vibrations;Input shaping;Key systems;Performance analyses;Phase plane;Residual vibrations;Simple methods;Vector diagram;Vibration analysis;Vibratory response;},
	number = {9},
	pages = {2392 - 2397},
	title = {Command-induced vibration analysis using input shaping principles},
	url = {http://dx.doi.org/10.1016/j.automatica.2008.01.029},
	volume = {44},
	year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBzLi4vRG9jdW1lbnRzL1Jlc2VhcmNoL0JpYmxpb2dyYXBoaWVzL3BhcGVycy9Tb3JlbnNlbiAtIENvbW1hbmQtaW5kdWNlZCB2aWJyYXRpb24gYW5hbHlzaXMgdXNpbmcgaW5wdXQgLSAyMDA4XzAwLnBkZk8RAmQAAAAAAmQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9Tb3JlbnNlbiAtIENvbW1hbmQjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABQAACiBjdQAAAAAAAAAAAAAAAAAGcGFwZXJzAAIAfS86VXNlcnM6am9zaDpEb2N1bWVudHM6UmVzZWFyY2g6QmlibGlvZ3JhcGhpZXM6cGFwZXJzOlNvcmVuc2VuIC0gQ29tbWFuZC1pbmR1Y2VkIHZpYnJhdGlvbiBhbmFseXNpcyB1c2luZyBpbnB1dCAtIDIwMDhfMDAucGRmAAAOAJAARwBTAG8AcgBlAG4AcwBlAG4AIAAtACAAQwBvAG0AbQBhAG4AZAAtAGkAbgBkAHUAYwBlAGQAIAB2AGkAYgByAGEAdABpAG8AbgAgAGEAbgBhAGwAeQBzAGkAcwAgAHUAcwBpAG4AZwAgAGkAbgBwAHUAdAAgAC0AIAAyADAAMAA4AF8AMAAwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgB7VXNlcnMvam9zaC9Eb2N1bWVudHMvUmVzZWFyY2gvQmlibGlvZ3JhcGhpZXMvcGFwZXJzL1NvcmVuc2VuIC0gQ29tbWFuZC1pbmR1Y2VkIHZpYnJhdGlvbiBhbmFseXNpcyB1c2luZyBpbnB1dCAtIDIwMDhfMDAucGRmAAATAAEvAAAVAAIAC///AAAACAANABoAJACaAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAwI=},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.automatica.2008.01.029}}

@article{Singer:90,
	abstract = {A method is presented for generating shaped command inputs which significantly reduce or eliminate endpoint vibration.  Desired system inputs are altered so that the system completes the requested move without residual vibration.  A short move time penalty is incurred (on the order of one period of the first mode of vibration).  The preshaping technique is robust under system paramenter uncertainty and may be applied to both open and closed loop systems.  the Draper Laboratory's Space Shuttle Remote Manipulator System simulator (DRS) is used to evaluate the method.  Results show a factor of 25 reduction in endpoint residual vibration for typical moves of the DRS.},
	annote = {Problem:  minimum-time point-to-point motion with positive impulses

Approach:  optimization of time-domain vibration equations

Robustness:  Y
Multiple Modes:  Y
System Type:  0

Comments prior to September 1995:
This paper outlines the fundamental theory of impulse shaping.

The introduction provides a good overview of past research in related techniques.},
	author = {Singer, Neil C. and Seering, Warren P.},
	date-added = {2021-04-29 04:08:53 -0400},
	date-modified = {2021-04-29 04:08:53 -0400},
	journal = {Journal of Dynamic Systems, Measurement, and Control},
	keywords = {Sequence Derivation Theory Multiple-mode Shaping ZVD},
	month = {March},
	pages = {76-82},
	title = {Preshaping Command Inputs to Reduce System Vibration},
	volume = {112},
	year = {1990},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBtLi4vRG9jdW1lbnRzL1Jlc2VhcmNoL0JpYmxpb2dyYXBoaWVzL3BhcGVycy9TaW5nZXIgLSBQcmVzaGFwaW5nIENvbW1hbmQgSW5wdXRzIHRvIFJlZHVjZSBTeXN0ZW0gLSAxOTkwXzAwLnBkZk8RAkwAAAAAAkwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9TaW5nZXIgLSBQcmVzaGFwaW4jRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABQAACiBjdQAAAAAAAAAAAAAAAAAGcGFwZXJzAAIAdy86VXNlcnM6am9zaDpEb2N1bWVudHM6UmVzZWFyY2g6QmlibGlvZ3JhcGhpZXM6cGFwZXJzOlNpbmdlciAtIFByZXNoYXBpbmcgQ29tbWFuZCBJbnB1dHMgdG8gUmVkdWNlIFN5c3RlbSAtIDE5OTBfMDAucGRmAAAOAIQAQQBTAGkAbgBnAGUAcgAgAC0AIABQAHIAZQBzAGgAYQBwAGkAbgBnACAAQwBvAG0AbQBhAG4AZAAgAEkAbgBwAHUAdABzACAAdABvACAAUgBlAGQAdQBjAGUAIABTAHkAcwB0AGUAbQAgAC0AIAAxADkAOQAwAF8AMAAwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgB1VXNlcnMvam9zaC9Eb2N1bWVudHMvUmVzZWFyY2gvQmlibGlvZ3JhcGhpZXMvcGFwZXJzL1NpbmdlciAtIFByZXNoYXBpbmcgQ29tbWFuZCBJbnB1dHMgdG8gUmVkdWNlIFN5c3RlbSAtIDE5OTBfMDAucGRmAAATAAEvAAAVAAIAC///AAAACAANABoAJACUAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAuQ=}}

@article{Singhose:94a,
	abstract = {This paper describes a method for limiting vibration in flexible systems by shaping the input to the system.  Unlike most previous input shaping strategies, this method does not require a precise system model or lengthy numerical computation; only estimates of the system natural frequency and damping ratio are required.  The effectiveness of this method when there are errors in the system model is explored and quantified.  Next, an algorithm is presented, which, given an upper bound on acceptable residual vibration amplitude, determines a shaping strategy that is insensitive to errors in the estimate of the natural frequency.  Finally, performance predictions are compared to hardware experiments.},
	annote = {NOT READ YET
NO HARDCOPY

Bill's Notes:
Vector diagram design of input shapers is discussed.  Using the vector diagram, input shapers with skewed insensitivity are designed. Extra-insensitive shapers are discussed.  The effects of damping are considered and curve fits to damped EI contraints are presented.  Experimental results verify vibration reduction and shape of extra-insensitive sensitivity curve.},
	author = {Singhose, William and Seering, Warren and Singer, Neil},
	date-added = {2021-04-29 04:08:44 -0400},
	date-modified = {2021-04-29 04:08:44 -0400},
	journal = {ASME J. of Mechanical Design},
	month = {June},
	pages = {654-659},
	title = {Residual Vibration Reduction Using Vector Diagrams to Generate Shaped Inputs},
	volume = {116},
	year = {1994},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBuLi4vRG9jdW1lbnRzL1Jlc2VhcmNoL0JpYmxpb2dyYXBoaWVzL3BhcGVycy9TaW5naG9zZSAtIFJlc2lkdWFsIFZpYnJhdGlvbiBSZWR1Y3Rpb24gVXNpbmcgVmVjdG9yIC0gMTk5NF8wMC5wZGZPEQJOAAAAAAJOAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fU2luZ2hvc2UgLSBSZXNpZHVhI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAABQREYgUERGcAABAAUAAAogY3UAAAAAAAAAAAAAAAAABnBhcGVycwACAHgvOlVzZXJzOmpvc2g6RG9jdW1lbnRzOlJlc2VhcmNoOkJpYmxpb2dyYXBoaWVzOnBhcGVyczpTaW5naG9zZSAtIFJlc2lkdWFsIFZpYnJhdGlvbiBSZWR1Y3Rpb24gVXNpbmcgVmVjdG9yIC0gMTk5NF8wMC5wZGYADgCGAEIAUwBpAG4AZwBoAG8AcwBlACAALQAgAFIAZQBzAGkAZAB1AGEAbAAgAFYAaQBiAHIAYQB0AGkAbwBuACAAUgBlAGQAdQBjAHQAaQBvAG4AIABVAHMAaQBuAGcAIABWAGUAYwB0AG8AcgAgAC0AIAAxADkAOQA0AF8AMAAwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgB2VXNlcnMvam9zaC9Eb2N1bWVudHMvUmVzZWFyY2gvQmlibGlvZ3JhcGhpZXMvcGFwZXJzL1Npbmdob3NlIC0gUmVzaWR1YWwgVmlicmF0aW9uIFJlZHVjdGlvbiBVc2luZyBWZWN0b3IgLSAxOTk0XzAwLnBkZgATAAEvAAAVAAIAC///AAAACAANABoAJACVAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAuc=}}
@article{Dwiel2019d,
abstract = {The use of robotics in controlled environments has flourished over the last several decades and training robots to perform tasks using control strategies developed from dynamical models of their hardware have proven very effective. However, in many real-world settings, the uncertainties of the environment, the safety requirements and generalized capabilities that are expected of robots make rigid industrial robots unsuitable. This created great research interest in developing control strategies for flexible robot hardware for which building dynamical models are challenging. In this paper, inspired by the success of deep reinforcement learning (DRL), we systematically study the efficacy of policy search methods using DRL in training flexible robots. Our results indicate that DRL is successfully able to learn efficient and robust policies for complex tasks at various degrees of flexibility. We also note that DRL using Deep Deterministic Policy Gradients can be sensitive to the choice of sensors and adding more informative sensors does not necessarily make the task easier to learn.},
archivePrefix = {arXiv},
arxivId = {1907.00269},
author = {Dwiel, Zach and Candadai, Madhavun and Phielipp, Mariano},
doi = {10.1109/IROS40897.2019.8968516},
eprint = {1907.00269},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Dwiel, Candadai, Phielipp - 2019 - On Training Flexible Robots using Deep Reinforcement Learning(2).pdf:pdf},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4666--4671},
title = {{On Training Flexible Robots using Deep Reinforcement Learning}},
year = {2019}
}
@article{Auerbach2014,
abstract = {Whether, when, how, and why increased complexity evolves in biological populations is a longstanding open question. In this work we combine a recently developed method for evolving virtual organisms with an information-theoretic metric of morphological complexity in order to investigate how the complexity of morphologies, which are evolved for locomotion, varies across different environments. We first demonstrate that selection for locomotion results in the evolution of organisms with morphologies that increase in complexity over evolutionary time beyond what would be expected due to random chance. This provides evidence that the increase in complexity observed is a result of a driven rather than a passive trend. In subsequent experiments we demonstrate that morphologies having greater complexity evolve in complex environments, when compared to a simple environment when a cost of complexity is imposed. This suggests that in some niches, evolution may act to complexify the body plans of organisms while in other niches selection favors simpler body plans. {\textcopyright} 2014 Auerbach, Bongard.},
annote = {cite on ES being used to change a robot structures},
author = {Auerbach, Joshua E. and Bongard, Josh C.},
doi = {10.1371/journal.pcbi.1003399},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/ContentServer.pdf:pdf},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {1},
pmid = {24391483},
title = {{Environmental Influence on the Evolution of Morphological Complexity in Machines}},
volume = {10},
year = {2014}
}
@article{Saranli2001,
annote = {advantegous legged systems},
author = {Saranli, U and Buehler, M and Koditschek, D E},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/saranli{\_}uluc{\_}2001{\_}1.pdf:pdf},
journal = {International Journal of Robotics Research},
keywords = {autonomy,biomimesis,clock,driven,hexapod robot,legged locomotion,mobility},
number = {7},
pages = {616--631},
title = {{RHex: A Simple and Highly Mobile Robot}},
volume = {20},
year = {2001}
}
@article{Hurst2010,
author = {Hurst, Jonathan W and Chestnutt, Joel E and Rizzi, Alfred A},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/05504832.pdf:pdf},
number = {4},
pages = {597--606},
title = {{Series Compliance}},
volume = {26},
year = {2010}
}
@article{Cheney2016,
abstract = {In this work we introduce a novel method for creating behaviors in cellular automata: optimizing the topology of the cellular substrate while maintaining a single simple update rule. We study the effect of altering the shape of a 3D cellular automaton and local signaling ability of each of its cells on the ability of that automaton as a whole to give rise to emergent locomotion behavior. This system optimizes for the physically embodied interactions between a cellular automaton with an external physically simulated world, rather than optimizing directly for a computational ability internal to the automaton itself. We give each cell in the automaton the ability to have an internal "excited" state, and also the ability to perform a physical action (volumetric contraction and expansion) as a result of that state. We then employ an evolutionary algorithm to optimize for the locomotion ability of the "robot" resulting from the behavior of this embodied automaton. We demonstrate a number of diverse topologies which lead to effective locomotion behaviors in this paradigm. We believe that creating complex behavior from simple rules in a complex substrate not only opens up questions about cellular automata, but also provides insights towards the study of morphological computation and embodied cognition.},
author = {Cheney, Nick and Lipson, Hod},
doi = {10.1016/j.tcs.2015.06.024},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/2016{\_}CheneyLipson{\_}TopologicalEvolutionForEmbodiedCellularAutomata.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Artificial life,Cellular automata,Complex systems,Embodied cognition,Evolutionary computation,Non-uniform,Topological optimization},
pages = {19--27},
publisher = {Elsevier B.V.},
title = {{Topological evolution for embodied cellular automata}},
url = {http://dx.doi.org/10.1016/j.tcs.2015.06.024},
volume = {633},
year = {2016}
}
@article{Li2001,
abstract = {The well-accepted basis for developing a mechatronic system is a synergetic concurrent design process that integrates different engineering disciplines. In this paper, a general model is derived to mathematically describe the concurrent design of a mechatronic system. Based on this model, a concurrent engineering approach, coined Design For Control (DFC), is formally presented for mechatronic systems design. Compared to other mechatronic design methodologies, DFC emphasizes obtaining a simple dynamic model of the mechanical structure by a judicious structure design and a careful selection of mechanical parameters. Once the simple dynamic model is available, in spite of the complexity of the mechanical structure, the controller design can be facilitated and better control performance can be achieved. Four design scenarios in application of DFC are addressed. A case study is implemented to demonstrate the effectiveness of DFC through the design and control of a programmable four-bar linkage.},
author = {Li, Q. and Zhang, W. J. and Chen, L.},
doi = {10.1109/3516.928731},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/00928731.pdf:pdf},
issn = {10834435},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Design methodology,Mechanism design,Mechatronic systems,Motion control},
number = {2},
pages = {161--169},
title = {{Design for control - A concurrent engineering approach for mechatronic systems design}},
volume = {6},
year = {2001}
}
@article{Wang2019,
abstract = {Despite the recent successes in robotic locomotion control, the design of robots, i.e., the design of their body structure, still heavily relies on human engineering. Automatic robot design has been a long studied subject, however, progress has been slow due to large combinatorial search space and the difficulty to efficiently evaluate the candidate structures. Note that one needs to both, search over many possible body structures, and choose among them based on how the robot with that structure performs in an environment. The latter means training an optimal controller given a candidate structure, which in itself is costly to obtain. In this paper, we propose Neural Graph Evolution (NGE), which performs evolutionary search in graph space, by iteratively evolving graph structures using simple mutation primitives. Key to our approach is to parameterize the control policies with graph neural networks, which allows us to transfer skills from previously evaluated designs during the graph search. This significantly reduces evaluation cost of new candidates and makes the search process orders of magnitude more efficient than that of past work. In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. We show that NGE significantly outperforms previous methods in terms of convergence rate and final performance. As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetric flat side-fins and a tail, or a cheetah with athletic front and back legs. NGE is extremely efficient, it finds plausible robotic structures within a day on a single 64 CPU-core Amazon EC2 machine. The code and project webpage are released1.},
annote = {Design parameters},
archivePrefix = {arXiv},
arxivId = {1906.05370},
author = {Wang, Tingwu and Zhou, Yuhao and Fidler, Sanja and Ba, Jimmy},
eprint = {1906.05370},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/1906.05370.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--17},
title = {{Neural graph evolution: Towards efficient automatic robot design}},
year = {2019}
}
@article{Peng2020,
abstract = {—Reproducing the diverse and agile locomotion skills of animals has been a longstanding challenge in robotics. While manually-designed controllers have been able to emulate many complex behaviors, building such controllers involves a time-consuming and difficult development process, often requiring substantial expertise of the nuances of each skill. Reinforcement learning provides an appealing alternative for automating the manual effort involved in the development of controllers. However, designing learning objectives that elicit the desired behaviors from an agent can also require a great deal of skill-specific expertise. In this work, we present an imitation learning system that enables legged robots to learn agile locomotion skills by imitating real-world animals. We show that by leveraging reference motion data, a single learning-based approach is able to automatically synthesize controllers for a diverse repertoire behaviors for legged robots. By incorporating sample efficient domain adaptation techniques into the training process, our system is able to learn adaptive policies in simulation that can then be quickly adapted for real-world deployment. To demonstrate the effectiveness of our system, we train an 18-DoF quadruped robot to perform a variety of agile behaviors ranging from different locomotion gaits to dynamic hops and turns. (Video1)},
archivePrefix = {arXiv},
arxivId = {arXiv:2004.00784v3},
author = {Peng, Xue Bin and Coumans, Erwin and Zhang, Tingnan and Lee, Tsang Wei Edward and Tan, Jie and Levine, Sergey},
doi = {10.15607/rss.2020.xvi.064},
eprint = {arXiv:2004.00784v3},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/2004.00784.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Learning agile robotic locomotion skills by imitating animals}},
year = {2020}
}
@article{Cheney2014,
abstract = {In 1994, Karl Sims' evolved virtual creatures showed the potential of evolutionary algorithms to produce natural, complex morphologies and behaviors [30]. One might assume that nearly 20 years of improvements in computational speed and evolutionary algorithms would produce far more impressive organisms, yet the creatures evolved in the field of artificial life today are not obviously more complex, natural, or intelligent. Fig. 2 demonstrates an example of similar complexity in robots evolved 17 years apart.},
author = {Cheney, Nick and MacCurdy, Robert and Clune, Jeff and Lipson, Hod},
doi = {10.1145/2661735.2661737},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/2661735.2661737.pdf:pdf},
isbn = {9781450319638},
journal = {ACM SIGEVOlution},
number = {1},
pages = {11--23},
title = {{Unshackling Evolution: Evolving Soft Robots with Multiple Materials and a Powerful Generative Encoding}},
volume = {7},
year = {2014}
}
@article{Buondonno2017,
abstract = {We present an optimization framework for the design and analysis of underactuated biped walkers, characterized by passive or actuated joints with rigid or non-negligible elastic actuation/transmission elements. The framework is based on optimal control, dealing with geometric constraints and various dynamic objective functions, as well as boundary conditions, which helps in selecting optimal values both for the actuation and the transmission parameters. Solutions of the formulated problems are shown for different kinds of bipedal architectures, and comparisons drawn between traditional rigid robots and compliant ones show the energy-efficiency of compliant actuators in the context of locomotion.},
annote = {See title

read more},
author = {Buondonno, Gabriele and Carpentier, Justin and Saurel, Guilhem and Mansard, Nicolas and {De Luca}, Alessandro and Laumond, Jean Paul},
doi = {10.1109/IROS.2017.8202228},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/08202228.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {705--711},
title = {{Actuator design of compliant walkers via optimal control}},
volume = {2017-Septe},
year = {2017}
}
@article{Bhagat2019e,
abstract = {The increasing trend of studying the innate softness of robotic structures and amalgamating it with the benefits of the extensive developments in the field of embodied intelligence has led to the sprouting of a relatively new yet rewarding sphere of technology in intelligent soft robotics. The fusion of deep reinforcement algorithms with soft bio-inspired structures positively directs to a fruitful prospect of designing completely self-sufficient agents that are capable of learning from observations collected from their environment. For soft robotic structures possessing countless degrees of freedom, it is at times not convenient to formulate mathematical models necessary for training a deep reinforcement learning (DRL) agent. Deploying current imitation learning algorithms on soft robotic systems has provided competent results. This review article posits an overview of various such algorithms along with instances of being applied to real-world scenarios, yielding frontier results. Brief descriptions highlight the various pristine branches of DRL research in soft robotics.},
author = {Bhagat, Sarthak and Banerjee, Hritwick and Tse, Zion Tsz Ho and Ren, Hongliang},
doi = {10.3390/robotics8010004},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Bhagat et al. - 2019 - Deep reinforcement learning for soft, flexible robots Brief review with impending challenges(3).pdf:pdf},
issn = {22186581},
journal = {Robotics},
keywords = {Deep reinforcement learning,Imitation learning,Soft robotics},
number = {1},
pages = {1--36},
title = {{Deep reinforcement learning for soft, flexible robots: Brief review with impending challenges}},
volume = {8},
year = {2019}
}
@article{Zhang2019,
abstract = {Transparency and guaranteed safety are important requirements in the design of wearable exoskeleton actuators for individuals who have lower limb deficits but still maintain a certain level of voluntary motor control. Specifically, precision in torque delivery timing and magnitude, robustness, disturbance rejection, and repeatability are desired in the actuator design and control. Motivated by these needs, this study aims to develop a series of elastic actuators with clutch (SEAC) that can precisely generate the desired assistance in terms of both timing and torque magnitude for a wearable hip exoskeleton and guarantee the wearer's safety at the same time. The proposed mechanical design improves actuator transparency and safety by a mechanical clutch that automatically disengages the transmission when needed. A new torque control for the SEAC, based on singular perturbation theory with flexible compensation techniques, is proposed to precisely control the assistive torque by rejecting the undesired human motion disturbance. The mechanical design of the proposed device and the design of a singular perturbation control algorithm are discussed, and the SEAC performance is verified by experiments. Experimental results, derived from a test with a human subject, are presented to demonstrate the precision of the assistive torque and timing control of the SEAC while interacting with a human wearer.},
annote = {series elastic actuator use cite},
author = {Zhang, Ting and Huang, He},
doi = {10.1109/TMECH.2019.2932312},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/08782586.pdf:pdf},
issn = {1941014X},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Series elastic actuator with clutch (SEAC),singular perturbation control,torque control,wearable hip exoskeleton},
number = {5},
pages = {2215--2226},
publisher = {IEEE},
title = {{Design and Control of a Series Elastic Actuator with Clutch for Hip Exoskeleton for Precise Assistive Magnitude and Timing Control and Improved Mechanical Safety}},
volume = {24},
year = {2019}
}
@article{Haarnoja2019a,
abstract = {Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.},
archivePrefix = {arXiv},
arxivId = {1812.11103},
author = {Haarnoja, Tuomas and Ha, Sehoon and Zhou, Aurick and Tan, Jie and Tucker, George and Levine, Sergey},
doi = {10.15607/rss.2019.xv.011},
eprint = {1812.11103},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Haarnoja et al. - 2019 - Learning to Walk Via Deep Reinforcement Learning.pdf:pdf},
title = {{Learning to Walk Via Deep Reinforcement Learning}},
year = {2019}
}
@article{Fankhauser2013,
abstract = {This paper presents the application of reinforcement learning to improve the performance of highly dynamic single legged locomotion with compliant series elastic actuators. The goal is to optimally exploit the capabilities of the hardware in terms of maximum jump height, jump distance, and energy efficiency of periodic hopping. These challenges are tackled with the reinforcement learning method Policy Improvement with Path Integrals (PI2) in a model-free approach to learn parameterized motor velocity trajectories as well as highlevel control parameters. The combination of simulation and hardware-based optimization allows to efficiently obtain optimal control policies in an up to 10-dimensional parameter space. The robotic leg learns to temporarily store energy in the elastic elements of the joints in order to improve the jump height and distance. In addition, we present a method to learn time-independent control policies and apply it to improve the energetic efficiency of periodic hopping. {\textcopyright} 2013 IEEE.},
author = {Fankhauser, Peter and Hutter, Marco and Gehring, Christian and Bloesch, Michael and Hoepflinger, Mark A. and Siegwart, Roland},
doi = {10.1109/IROS.2013.6696352},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/06696352.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {188--193},
title = {{Reinforcement learning of single legged locomotion}},
year = {2013}
}
@article{Lillicrap2016h,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
annote = {DDPG Original Paper},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Lillicrap et al. - 2016 - Continuous control with deep reinforcement learning.pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
title = {{Continuous control with deep reinforcement learning}},
year = {2016}
}
@article{Hu2020,
abstract = {We consider the problem of optimizing a robot morphology to achieve the best performance for a target task, under computational resource limitations. The evaluation process for each morphological design involves learning a controller for the design, which can consume substantial time and computational resources. To address the challenge of expensive robot morphology evaluation, we present a continuous multi-fidelity Bayesian Optimization framework that efficiently utilizes computational resources via low-fidelity evaluations. We identify the problem of non-stationarity over fidelity space. Our proposed fidelity warping mechanism can learn representations of learning epochs and tasks to model non-stationary covariances between continuous fidelity evaluations which prove challenging for off-the-shelf stationary kernels. Various experiments demonstrate that our method can utilize the low-fidelity evaluations to efficiently search for the optimal robot morphology, outperforming state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {2012.04195},
author = {Hu, Sha and Yang, Zeshi and Mori, Greg},
eprint = {2012.04195},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/2012.04195.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Neural fidelity warping for efficient robot morphology design}},
year = {2020}
}
@article{Pratt1995,
abstract = {It is traditional to make the interface between an actuator and its load as stiff as possible. Despite this tradition, reducing interface stiffness offers a number of advantages, including greater shock tolerance, lower reflected inertia, more accurate and stable force control, less inadvertent damage to the environment, and the capacity for energy storage. As a trade-off, reducing interface stiffness also lowers zero motion force bandwidth. In this paper, we propose that for natural tasks, zero motion force bandwidth isn't everything, and incorporating series elasticity as a purposeful element within an actuator is a good idea. We use the term elasticity instead of compliance to indicate the presence of a passive mechanical spring in the actuator. After a discussion of the trade-offs inherent in series elastic actuators, we present a control system for their use under general force or impedance control. We conclude with test results from a revolute series-elastic actuator meant for the arms of the MIT humanoid robot Cog and for a small planetary rover.},
author = {Pratt, Gill A. and Williamson, Matthew M.},
doi = {10.1109/iros.1995.525827},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/SEA{\_}Pratt.pdf:pdf},
isbn = {0818671084},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {399--406},
title = {{Series elastic actuators}},
volume = {1},
year = {1995}
}
@article{Salimans2017g,
abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
annote = {From Vaughan},
archivePrefix = {arXiv},
arxivId = {1703.03864},
author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
eprint = {1703.03864},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Salimans et al. - 2017 - Evolution strategies as a scalable alternative to reinforcement learning.pdf:pdf},
journal = {arXiv},
pages = {1--13},
title = {{Evolution strategies as a scalable alternative to reinforcement learning}},
year = {2017}
}
@article{Ha2019j,
abstract = {In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agentʼs physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agentʼs design that is better suited for its task, jointly with the policy. We propose an alteration to the popular OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications.},
annote = {From Vaughan},
archivePrefix = {arXiv},
arxivId = {1810.03779},
author = {Ha, David},
doi = {10.1162/artl_a_00301},
eprint = {1810.03779},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Ha - 2019 - Reinforcement learning for improving agent design(2).pdf:pdf},
issn = {15309185},
journal = {Artificial Life},
keywords = {Deep reinforcement learning,Evolution strategies,Generative design,Neuroevolution},
number = {4},
pages = {352--365},
pmid = {31697584},
title = {{Reinforcement learning for improving agent design}},
volume = {25},
year = {2019}
}
@article{Pradhan2012e,
abstract = {This paper exploits reinforcement learning (RL) for developing real-time adaptive control of tip trajectory and deflection of a two-link flexible manipulator handling variable payloads. This proposed adaptive controller consists of a proportional derivative (PD) tracking loop and an actor-critic-based RL loop that adapts the actor and critic weights in response to payload variations while suppressing the tip deflection and tracking the desired trajectory. The actor-critic-based RL loop uses a recursive least square (RLS)-based temporal difference (TD) learning with eligibility trace and an adaptive memory to estimate the critic weights and a gradient-based estimator for estimating actor weights. Tip trajectory tracking and suppression of tip deflection performances of the proposed RL-based adaptive controller (RLAC) are compared with that of a nonlinear regression-based direct adaptive controller (DAC) and a fuzzy learning-based adaptive controller (FLAC). Simulation and experimental results envisage that the RLAC outperforms both the DAC and FLAC. {\textcopyright} 2012 IEEE.},
author = {Pradhan, Santanu Kumar and Subudhi, Bidyadhar},
doi = {10.1109/TASE.2012.2189004},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Pradhan, Subudhi - 2012 - Real-time adaptive control of a flexible manipulator using reinforcement learning(2).pdf:pdf},
issn = {15455955},
journal = {IEEE Transactions on Automation Science and Engineering},
keywords = {Adaptive control,flexible-link manipulator,reinforcement learning,tip trajectory tracking},
number = {2},
pages = {237--249},
publisher = {IEEE},
title = {{Real-time adaptive control of a flexible manipulator using reinforcement learning}},
volume = {9},
year = {2012}
}
@article{Whitman2020,
abstract = {Modular robots hold the promise of versatility in that their components can be re-arranged to adapt the robot design to a task at deployment time. Even for the simplest designs, determining the optimal design is exponentially complex due to the number of permutations of ways the modules can be connected. Further, when selecting the design for a given task, there is an additional computational burden in evaluating the capability of each robot, e.g., whether it can reach certain points in the workspace. This work uses deep reinforcement learning to create a search heuristic that allows us to efficiently search the space of modular serial manipulator designs. We show that our algorithm is more computationally efficient in determining robot designs for given tasks in comparison to the current state-of-the-art.},
author = {Whitman, Julian and Bhirangi, Raunaq and Travers, Matthew and Choset, Howie},
doi = {10.1609/aaai.v34i06.6611},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/6611-Article Text-9839-1-10-20200520.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {Robotics},
number = {06},
pages = {10418--10425},
title = {{Modular Robot Design Synthesis with Deep Reinforcement Learning}},
volume = {34},
year = {2020}
}
@article{Ghorbel1990,
abstract = {This paper presents an adap- tive control result for flexible-joint robot ma- nipulators. Under the assumption of weak joint elasticity, a singular perturbation ar- gument is used to show that recent adaptive control results for rigid robots may be used to control flexible-joint robots provided a simple correction term is added to the control law to damp out the elastic oscillations at the joints. In this way, fundamental properties of rigid robot dynamics may be exploited to design adaptive control laws for flexible-joint robots that are robust to parametric uncer- tainty. Experimental results are given to il- lustrate the theory.},
author = {Ghorbel, Fathi and Hung, John Y and Spong, Mark W},
doi = {10.1109/ICSMC.1990.142189},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Adaptive{\_}control{\_}of{\_}flexible{\_}joint{\_}manipulators.pdf:pdf},
number = {December},
title = {{Adaptive Control of Flexible-Joint Manipulators}},
year = {1990}
}
@article{Liao2019,
abstract = {Robot design is often a slow and difficult process requiring the iterative construction and testing of prototypes, with the goal of sequentially optimizing the design. For most robots, this process is further complicated by the need, when validating the capabilities of the hardware to solve the desired task, to already have an appropriate controller, which is in turn designed and tuned for the specific hardware. In this paper, we propose a novel approach, HPC-BBO, to efficiently and automatically design hardware configurations, and evaluate them by also automatically tuning the corresponding controller. HPC-BBO is based on a hierarchical Bayesian optimization process which iteratively optimizes morphology configurations (based on the performance of the previous designs during the controller learning process) and subsequently learns the corresponding controllers (exploiting the knowledge collected from optimizing for previous morphologies). Moreover, HPC-BBO can select a “batch” of multiple morphology designs at once, thus parallelizing hardware validation and reducing the number of time-consuming production cycles. We validate HPC-BBO on the design of the morphology and controller for a simulated 6-legged microrobot. Experimental results show that HPC-BBO outperforms multiple competitive baselines, and yields a 360{\%} reduction in production cycles over standard Bayesian optimization, thus reducing the hypothetical manufacturing time of our microrobot from 21 to 4 months.},
author = {Liao, Thomas and Wang, Grant and Yang, Brian and Lee, Rene and Pister, Kristofer and Levine, Sergey and Calandra, Roberto},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/08793802.pdf:pdf},
isbn = {9781538660270},
issn = {23318422},
journal = {arXiv},
pages = {2488--2494},
title = {{Data-efficient learning of morphology and controller for a microrobot}},
year = {2019}
}
@article{Chen2020,
abstract = {Deep Reinforcement Learning (RL) has shown great success in learning complex control policies for a variety of applications in robotics. However, in most such cases, the hardware of the robot has been considered immutable, modeled as part of the environment. In this study, we explore the problem of learning hardware and control parameters together in a unified RL framework. To achieve this, we propose to model aspects of the robot's hardware as a “mechanical policy”, analogous to and optimized jointly with its computational counterpart. We show that, by modeling such mechanical policies as auto-differentiable computational graphs, the ensuing optimization problem can be solved efficiently by gradient-based algorithms from the Policy Optimization family. We present two such design examples: a toy mass-spring problem, and a real-world problem of designing an underactuated hand. We compare our method against traditional co-optimization approaches, and also demonstrate its effectiveness by building a physical prototype based on the learned hardware parameters.},
archivePrefix = {arXiv},
arxivId = {2008.04460},
author = {Chen, Tianjian and He, Zhanpeng and Ciocarlie, Matei},
eprint = {2008.04460},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/2008.04460.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Mechanical-Computational Co-Optimization,Reinforcement Learning},
number = {CoRL},
title = {{Hardware as Policy: Mechanical and computational co-optimization using deep reinforcement learning}},
year = {2020}
}
@article{Yang2019,
abstract = {We present a model-based reinforcement learning framework for robot locomotion that achieves walking based on only 4.5 minutes (45,000 control steps) of data collected on a quadruped robot. To accurately model the robot's dynamics over a long horizon, we introduce a loss function that tracks the model's prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function. To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.},
annote = {Look into more},
archivePrefix = {arXiv},
arxivId = {1907.03613},
author = {Yang, Yuxiang and Caluwaerts, Ken and Iscen, Atil and Zhang, Tingnan and Tan, Jie and Sindhwani, Vikas},
eprint = {1907.03613},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/yang20a.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Legged Locomotion,Model Predictive Control,Model-based Reinforcement Learning},
number = {CoRL},
pages = {1--10},
title = {{Data efficient reinforcement learning for legged robots}},
year = {2019}
}
@article{Iida2005,
abstract = {Exploiting the body dynamics to control the behavior of robots is one of the most challenging issues, because the use of body dynamics has a significant potential in order to enhance both complexity of the robot design and the speed of movement. In this paper, we explore the control strategy of rapid four-legged locomotion by exploiting the intrinsic body dynamics. Based on the fact that a simple model of four-legged robot is known to exhibit interesting locomotion behavior, this paper analyzes the characteristics of the dynamic locomotion for the purpose of the locomotion control. The results from a series of running experiments with a robot show that, by exploiting the unique characteristics induced by the body dynamics, the forward velocity can be controlled by using a very simple method, in which only one control parameter is required. Furthermore it is also shown that a few of such different control parameters exist, each of them can control the forward velocity. Interestingly, with these parameters, the robot exhibits qualitatively different behavior during the locomotion, which could lead to our comprehensive understanding toward the behavioral diversity of adaptive robotic systems. {\textcopyright} 2005 IEEE.},
annote = {series elastic actuator},
author = {Iida, Fumiya and G{\'{o}}mez, Gabriel and Pfeifer, Rolf},
doi = {10.1109/ICAR.2005.1507417},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/01507417.pdf:pdf},
isbn = {0780391772},
journal = {2005 International Conference on Advanced Robotics, ICAR '05, Proceedings},
pages = {229--235},
publisher = {IEEE},
title = {{Exploiting body dynamics for controlling a running quadruped robot}},
volume = {2005},
year = {2005}
}
@article{Cui2019e,
abstract = {Most researches about control of flexible manipulators are all based on the dynamic model, which is difficult to establish because of their flexibility and the tedious process of measuring flexible link's parameters. In this paper, the goal is to design a controller which is able to control the flexible manipulator to track a given position in joint space and suppress vibration without knowing the dynamic model. For the problem of tracking a given position, a tracking controller is designed based on sliding mode control, and for the purpose of vibration suppression, a vibration suppression controller is designed as a deep neural network. Because the input of the flexible manipulator, torques at each joint, is a high dimensional and continuous space, Deep Deterministic Policy Gradient Algorithm (DDPG) is adopted to train the neural network in the vibration suppression controller. The effectiveness of the proposed controller to track a given position and suppress vibration is demonstrated by numerical simulation.},
author = {Cui, Leilei and Chen, Weidong and Wang, Hesheng and Wang, Jingchuan},
doi = {10.1109/CAC.2018.8623788},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Cui et al. - 2019 - Control of Flexible Manipulator Based on Reinforcement Learning(2).pdf:pdf},
journal = {Proceedings 2018 Chinese Automation Congress, CAC 2018},
keywords = {flexible manipulator,reinforcement learning,vibration suppression},
pages = {2744--2749},
publisher = {IEEE},
title = {{Control of Flexible Manipulator Based on Reinforcement Learning}},
year = {2019}
}
@article{Real2019b,
abstract = {We propose V2CNet, a new deep learning framework to automatically translate the demonstration videos to commands that can be directly used in robotic applications. Our V2CNet has two branches and aims at understanding the demonstration video in a fine-grained manner. The first branch has the encoder-decoder architecture to encode the visual features and sequentially generate the output words as a command, while the second branch uses a Temporal Convolutional Network (TCN) to learn the fine-grained actions. By jointly training both branches, the network is able to model the sequential information of the command, while effectively encodes the fine-grained actions. The experimental results on our new large-scale dataset show that V2CNet outperforms recent state-of-the-art methods by a substantial margin, while its output can be applied in real robotic applications. The source code and trained models will be made available.},
author = {Real, Fabio and Batou, Anas and Ritto, Thiago and Desceliers, Christophe},
doi = {10.1177/ToBeAssigned},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/IJRR{\_}2018.pdf:pdf},
journal = {Journal of Vibration and Control},
keywords = {[PHYS.MECA.VIBR]Physics [physics]/Mechanics [physi,bit-rock stochastic interaction model,drillstring dynamics,experimental identification,hysteretic friction},
pages = {107754631982824},
title = {{Stochastic modeling for hysteretic bit–rock interaction of a drill string under torsional vibrations}},
year = {2019}
}
@article{Schaff2019e,
abstract = {The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learning-based approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical - i.e., by picking a design and training a control policy for it. Since training these policies is time-consuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that jointly optimizes over the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines across different settings.},
archivePrefix = {arXiv},
arxivId = {1801.01432},
author = {Schaff, Charles and Yunis, David and Chakrabarti, Ayan and Walter, Matthew R.},
doi = {10.1109/ICRA.2019.8793537},
eprint = {1801.01432},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Schaff et al. - 2019 - Jointly learning to construct and control agents using deep reinforcement learning.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {9798--9805},
title = {{Jointly learning to construct and control agents using deep reinforcement learning}},
volume = {2019-May},
year = {2019}
}
@article{Luck2019,
abstract = {Humans and animals are capable of quickly learning new behaviours to solve new tasks. Yet, we often forget that they also rely on a highly specialized morphology that co-adapted with motor control throughout thousands of years. Although compelling, the idea of co-adapting morphology and behaviours in robots is often unfeasible because of the long manufacturing times, and the need to redesign an appropriate controller for each morphology. In this paper, we propose a novel approach to automatically and efficiently co-adapt a robot morphology and its controller. Our approach is based on recent advances in deep reinforcement learning, and specifically the soft actor critic algorithm. Key to our approach is the possibility of leveraging previously tested morphologies and behaviors to estimate the performance of new candidate morphologies. As such, we can make full use of the information available for making more informed decisions, with the ultimate goal of achieving a more data-efficient co-adaptation (i.e., reducing the number of morphologies and behaviors tested). Simulated experiments show that our approach requires drastically less design prototypes to find good morphology-behaviour combinations, making this method particularly suitable for future co-adaptation of robot designs in the real world.},
annote = {Learning design parameters},
archivePrefix = {arXiv},
arxivId = {1911.06832},
author = {Luck, Kevin Sebastian and Amor, Heni Ben and Calandra, Roberto},
eprint = {1911.06832},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/1911.06832.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Co-adaptation,Deep Reinforcement Learning,Morphology},
number = {CoRL},
title = {{Data-efficient co-adaptation of morphology and behaviour with deep reinforcement learning}},
year = {2019}
}
@article{Hurst2008,
abstract = {Many robots excel at precise positioning and trajectory tracking using software control, and most successful robotic applications utilize this ability-examples include CNC machining, robotic welding, painting, and pick-and-place circuit board assembly. The mechanical design of these robots focuses on rigid transmissions and minimizing compliance in the structure, so the software controller can accurately track a desired position as a function of time, regardless of any disturbance forces. However, there is a class of tasks for which rigid actuation is not ideal: physical interaction with the world, especially interaction that involves an impact or kinetic energy transfer. Animals tend to excel at these tasks, and far outperform the best robots. Examples include walking, running, catching a ball, gripping a piece of fruit firmly but without causing damage, and many types of assembly tasks. For dynamic behaviors such as running, the performance limitations of a robot are often due to limitations of the mechanical design. A robot is an integrated system of electronics, software, and mechanism, and each part of the system limits or enables the behavior of the whole. While some behaviors can easily be implemented through simple actuators and direct software control, a running machine requires mechanical design that is specialized for the task. Among other things, physical springs are essential for a robust and efficient running gait, to store energy, provide high mechanical power, and overcome bandwidth limitations of traditional actuators. An ideal kinematic design, where the joints and links are perfectly sized and placed for the desired task, and motors that exceed the force and speed requirements of the task are not sufficient for successful dynamic interactions. Inertia, transmission friction, and other dynamic effects have a significant role in the behavior of a robot. We are building running and walking machines with a focus on mechanical design to enable efficient and robust gaits. The defining characteristic of a running gait is spring-like behavior; all running animals, from small insects to large mammals, exhibit a center-of-mass motion that resembles a bouncing ball or a pogo stick. The spring-like behavior is implemented with the assistance of physical springy elements, such as tendons, and not entirely through software or neural control. Energy cycles back and forth between the ballistic trajectory of the body and the compression of the leg spring. To exhibit this behavior, our robots incorporate a mechanical spring that is tuned to absorb and release the energy of a running gait at the appropriate frequency. Electric motors act in series with this spring to add or remove energy from the cycle to modify or control the running gait. Our first prototype machine is a single actuator mounted to a bench, called the Actuator with Mechanically Adjustable Series Compliance, or AMASC. The stiffness and the no-load position of the joint are mechanical configurations that can be independently adjusted using two separate motors, and it is a test platform to verify and refine several design ideas for leg joints of running and walking robots. After significant testing and design, revision, we incorporated the ideas behind the AMASC into the design of a full bipedal robot, the Biped with Mechanically Adjustable Series Compliance, or BiMASC. A single leg prototype of the BiMASC was constructed and tested, and after some final revisions, we have built the Electric Cable Differential (ECD) Leg. The ECD Leg derives its name from the construction-using electric motors, cable drives, and mechanical differentials to actuate the system. One ECD Leg, named Thumper, is assembled as a monopod and installed in our laboratory at the Robotics Institute to study the role of compliance in running gaits. Two ECD Legs are assembled as a biped named MABEL, which is installed in Professor Jessy Grizzle's Laboratory at the University of Michigan and will serve as a platform to explore novel control ideas. In this thesis, we demonstrate that physical springs are extremely important for supporting a running gait. Additionally, through experiments on the ECD Leg, we demonstrate that there is an energetically optimal leg stiffness. The design and construction of the robots in this thesis are an exploration of methods for adjusting the leg stiffness to obtain the optimal stiffness for a running gait. The AMASC and BiMASC utilize co-contraction of antagonistic springs to tune the stiffness to a desired value. The ECD leg, in contrast, utilizes both active control of the motor and kinematic adjustments to control the stiffness behavior of the leg. We suggest that that co-contraction of antagonistic springs is energetically expensive, and that active control of a leg to modify its inherent stiffness might be energetically cheaper, with the same functionality. The ECD leg successfully hopped in place, as well as at speed. Thumper and MABEL demonstrate, for the first time, a cable drive paired with series elasticity for a running machine. The AMASC is the first demonstration of series elasticity implemented via a mechanical differential, and the BIMASC, Thumper, and MABEL all utilize this same concept. The mechanical design of the ECD leg contains many novel ideas that will be utilized in future walking and running machines.},
author = {Hurst, Jonathan},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/The{\_}Role{\_}and{\_}Implementation{\_}of{\_}Compliance{\_}in{\_}Legge.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
number = {4},
pages = {110},
pmid = {304667024},
title = {{The Role and Implementation of Compliance in Legged Locomotion}},
volume = {25},
year = {2008}
}
@article{Thuruthelb,
author = {Thuruthel, Thomas George and Falotico, Egidio and Renda, Federico and Laschi, Cecilia},
doi = {10.1109/TRO.2018.2878318},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Thuruthel et al. - Unknown - Model Based Reinforcement Learning for Closed Loop Dynamic Control of Soft Robotic Manipulators.pdf:pdf},
journal = {IEEE Transactions on Robotics},
pages = {124--134},
title = {{Model Based Reinforcement Learning for Closed Loop Dynamic Control of Soft Robotic Manipulators}},
volume = {35},
year = {2019}
}
@article{Fujimoto2018d,
abstract = {In value-based reinforcement learning methods such as deep Q-leaming, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit over- estimation. We draw the conncction between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
annote = {TD3 Original Paper},
archivePrefix = {arXiv},
arxivId = {1802.09477},
author = {Fujimoto, Scott and {Van Hoof}, Herke and Meger, David},
eprint = {1802.09477},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/1802.09477.pdf:pdf},
isbn = {9781510867963},
issn = {1938-7228},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {2587--2601},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
volume = {4},
year = {2018}
}
@article{Da2020c,
abstract = {We present a hierarchical framework that combines model-based control and reinforcement learning (RL) to synthesize robust controllers for a quadruped (the Unitree Laikago). The system consists of a high-level controller that learns to choose from a set of primitives in response to changes in the environment and a low-level controller that utilizes an established control method to robustly execute the primitives. Our framework learns a controller that can adapt to challenging environmental changes on the fly, including novel scenarios not seen during training. The learned controller is up to 85{\~{}}percent more energy efficient and is more robust compared to baseline methods. We also deploy the controller on a physical robot without any randomization or adaptation scheme.},
archivePrefix = {arXiv},
arxivId = {2009.10019},
author = {Da, Xingye and Xie, Zhaoming and Hoeller, David and Boots, Byron and Anandkumar, Animashree and Zhu, Yuke and Babich, Buck and Garg, Animesh},
eprint = {2009.10019},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Da et al. - 2020 - Learning a Contact-Adaptive Controller for Robust, Efficient Legged Locomotion.pdf:pdf},
keywords = {hierarchical control,legged locomotion,reinforcement learning},
number = {c},
title = {{Learning a Contact-Adaptive Controller for Robust, Efficient Legged Locomotion}},
url = {http://arxiv.org/abs/2009.10019},
volume = {1},
year = {2020}
}
@article{Vaughan2013,
abstract = {Under certain conditions, hopping, skipping, or jumping might be a more efficient means of terrestrial locomotion than either walking or rolling. In addition, for many robotic applications, a robust jumping algorithm could be easier to implement than walking. Jumping also provides a means to cross obstacles that might otherwise stop a wheeled or walking robot. This paper will examine a special case of jumping and generation of jumping commands. A simplified flexible robotic leg is modeled and jumping commands are generated utilizing a command shaping algorithm.},
annote = {cite for control input},
author = {Vaughan, Joshua},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/06695635.pdf:pdf},
isbn = {7777777777},
publisher = {IEEE},
title = {{Jumping Commands For Flexible-Legged Robots}},
year = {2013}
}
@article{Xiao2019,
abstract = {Model-based reinforcement learning methods provide a promising direction for a range of automated applications, such as autonomous vehicles and legged robots, due to their sample-efficiency. However, their asymptotic performance is usually inferior compared to the state-of-the-art model-free reinforcement learning methods in locomotion control domains. One main challenge of model-based reinforcement learning is learning a dynamics model that is accurate enough for planning. This paper mitigates this issue by meta-reinforcement learning from an ensemble of dynamics models. A policy learns from dynamics models that hold different beliefs of a real environment. This procedure improves its adaptability and inaccuracy-tolerance ability. A proximal meta-reinforcement learning algorithm is introduced to improve computational efficiency and reduces variance of higher-order gradient estimation. A heteroscedastic noise is added to the training dataset, thus leading to a robust and efficient model learning. Subsequently, proximal meta-reinforcement learning maximizes the expected returns by sampling 'imaginary' trajectories from the learned dynamics, which does not require real environment data and can be deployed on many servers in parallel to speed up the whole learning process. The aim of this work is to reduce the sample-complexity and computational cost of reinforcement learning in robot locomotion tasks. Simulation experiments show that the proposed algorithm achieves an asymptotic performance compared with the state-of-the-art model-free reinforcement learning methods with significantly fewer samples, which confirm our theoretical results.},
author = {Xiao, Qing and Cao, Zhengcai and Zhou, Mengchu},
doi = {10.1109/SMC.2019.8914406},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/08914406.pdf:pdf},
isbn = {9781728145693},
issn = {1062922X},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
pages = {1545--1550},
publisher = {IEEE},
title = {{Learning locomotion skills via model-based proximal meta-reinforcement learning}},
volume = {2019-Octob},
year = {2019}
}
@article{He2020f,
author = {He, Wei and Gao, Hejia and Zhou, Chen and Yang, Chenguang and Li, Zhijun},
doi = {10.1109/tsmc.2020.2975232},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/He et al. - 2020 - Reinforcement Learning Control of a Flexible Two-Link Manipulator An Experimental Investigation(2).pdf:pdf},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
pages = {1--11},
title = {{Reinforcement Learning Control of a Flexible Two-Link Manipulator: An Experimental Investigation}},
year = {2020}
}
@article{Hutter2013,
abstract = {— This paper presents the application of reinforce-ment learning to improve the performance of highly dynamic single legged locomotion with compliant series elastic actuators. The goal is to optimally exploit the capabilities of the hardware in terms of maximum jump height, jump distance, and energy efficiency of periodic hopping. These challenges are tackled with the reinforcement learning method Policy Improvement with Path Integrals (PI 2) in a model-free approach to learn parameterized motor velocity trajectories as well as high-level control parameters. The combination of simulation and hardware-based optimization allows to efficiently obtain optimal control policies in an up to 10-dimensional parameter space. The robotic leg learns to temporarily store energy in the elastic elements of the joints in order to improve the jump height and distance. In addition, we present a method to learn time-independent control policies and apply it to improve the energetic efficiency of periodic hopping.},
author = {Hutter, Marco and Bloesch, Michael and Siegwart, Roland},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Fankhauser{\_}2013{\_}DW.pdf:pdf},
isbn = {9781467363570},
journal = {Dynamic Walking},
pages = {188--193},
title = {{Reinforcement Learning of Robotic Legged Locomotion}},
year = {2013}
}
@article{Fadini2020,
annote = {This paper presented a computational method to design legged robots that can achieve a user-defined task with minimal energy consumption.},
author = {Fadini, Gabriele and Flayols, Thomas and Prete, Andrea and Mansard, Nicolas and Fadini, Gabriele and Flayols, Thomas and Prete, Andrea and Mansard, Nicolas and Computa-, Philippe Sou{\`{e}}res and Fadini, G and Flayols, T and Prete, A Del and Mansard, N and Sou, P},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/ICRA21{\_}fadini.pdf:pdf},
title = {{Computational design of energy-efficient legged robots : Optimizing for size and actuators To cite this version : HAL Id : hal-02993624 Computational design of energy-efficient legged robots : Optimizing for size and actuators}},
year = {2020}
}
@article{Modeling2003,
annote = {control of flexible systems},
author = {Modeling, A Mathematical},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/01225552.pdf:pdf},
isbn = {0780377591},
number = {AIM},
pages = {1423--1428},
title = {{Gain Adaptive Nonlinear Feedback Control of Flexible SCARA / Cartesian Robots}},
year = {2003}
}
@article{Brockman2016c,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
annote = {Describe the library.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Brockman et al. - 2016 - OpenAI Gym.pdf:pdf},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Ahmadi1997,
abstract = {We present a control strategy for a simplified model of a one-legged running robot which features compliant elements in series with hip and leg actuators. For this model, proper spring selection and initial conditions result in "passive dynamic" operation close to the desired motion, without any actuation. However, this motion is not stable. Our controller is based on online calculations of the desired passive dynamic motion which is then parametrized in terms of a normalized "locomotion time." We show in simulation that the proposed controller stabilizes a wide range of velocities and is robust to modeling errors. It also tracks changes in desired robot velocity and remains largely passive despite a fixed set of springs, masses, and inertias. Comparisons of simulated runs with direct hip actuation show 95{\%} hip actuation energy savings at 3 m/s. Such energy savings are critical for the power autonomy of electrically actuated legged robots.},
annote = {series elastic actuator},
author = {Ahmadi, Mojtaba and Buehler, Martin},
doi = {10.1109/70.554350},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/00554350.pdf:pdf},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {Legged locomotion,Passive dynamics,Robotics},
number = {1},
pages = {96--104},
title = {{Stable control of a simulated one-legged running robot with hip and leg compliance}},
volume = {13},
year = {1997}
}
@article{Sugiyama2004,
abstract = {We describe crawling and jumping by a deformable soft robot. Locomotion over rough terrain has been achieved mainly by rigid body systems including crawlers and leg mechanisms. This paper presents an alternative method of moving over rough terrain, one that employs deformation. First, we describe the principle of crawling and jumping as performed through deformation of a robot body. Second, in a physical simulation, we investigate the feasibility of the approach. Next, we show experimentally that a prototype of a circular soft robot can crawl and jump.},
annote = {cite for flexible systems being advantagous},
author = {Sugiyama, Yuuta and Hirai, Shinichi},
doi = {10.1109/iros.2004.1389922},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/01389922.pdf:pdf},
isbn = {0780384636},
journal = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
keywords = {Crawl,Deformation,Jump,Locomotion},
number = {c},
pages = {3276--3281},
title = {{Crawling and jumping of deformable soft robot}},
volume = {4},
year = {2004}
}
@article{Tsounis2019,
abstract = {This paper addresses the problem of legged locomotion in non-flat terrain. As legged robots such as quadrupeds are to be deployed in terrains with geometries which are difficult to model and predict, the need arises to equip them with the capability to generalize well to unforeseen situations. In this work, we propose a novel technique for training neural-network policies for terrain-aware locomotion, which combines state-of-the-art methods for model-based motion planning and reinforcement learning. Our approach is centered on formulating Markov decision processes using the evaluation of dynamic feasibility criteria in place of physical simulation. We thus employ policy-gradient methods to independently train policies which respectively plan and execute foothold and base motions in 3D environments using both proprioceptive and exteroceptive measurements. We apply our method within a challenging suite of simulated terrain scenarios which contain features such as narrow bridges, gaps and stepping-stones, and train policies which succeed in locomoting effectively in all cases.},
archivePrefix = {arXiv},
arxivId = {arXiv:1909.08399v2},
author = {Tsounis, Vassilios and Alge, Mitja and Lee, Joonho and Farshidian, Farbod and Hutter, Marco},
eprint = {arXiv:1909.08399v2},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/1909.08399.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {1},
pages = {1--8},
title = {{DeepGait: Planning and control of quadrupedal gaits using deep reinforcement learning}},
year = {2019}
}
@article{Lee2020a,
abstract = {Legged locomotion can extend the operational domain of robots to some of the most challenging environments on Earth. However, conventional controllers for legged locomotion are based on elaborate state machines that explicitly trigger the execution of motion primitives and reflexes. These designs have increased in complexity but fallen short of the generality and robustness of animal locomotion. Here, we present a robust controller for blind quadrupedal locomotion in challenging natural environments. Our approach incorporates proprioceptive feedback in locomotion control and demonstrates zero-shot generalization from simulation to natural environments. The controller is trained by reinforcement learning in simulation. The controller is driven by a neural network policy that acts on a stream of proprioceptive signals. The controller retains its robustness under conditions that were never encountered during training: deformable terrains such as mud and snow, dynamic footholds such as rubble, and overground impediments such as thick vegetation and gushing water. The presented work indicates that robust locomotion in natural environments can be achieved by training in simple domains.},
annote = {From Vaughan
Site: https://leggedrobotics.github.io/rl-blindloco/},
archivePrefix = {arXiv},
arxivId = {2010.11251},
author = {Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
doi = {10.1126/SCIROBOTICS.ABC5986},
eprint = {2010.11251},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Lee et al. - 2020 - Learning quadrupedal locomotion over challenging terrain.pdf:pdf},
issn = {24709476},
journal = {Science Robotics},
number = {47},
pages = {1--22},
pmid = {33087482},
title = {{Learning quadrupedal locomotion over challenging terrain}},
volume = {5},
year = {2020}
}
@article{Luo1993,
annote = {control of flexible systems},
author = {Luo, Zheng-hua},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/00262031.pdf:pdf},
number = {11},
title = {{Direct Strain Feedback Control of Flexible Robot Arms: New Theoretical and Experimental Results}},
volume = {38},
year = {1993}
}
@article{Reda2020c,
abstract = {Learning to locomote is one of the most common tasks in physics-based animation and deep reinforcement learning (RL). A learned policy is the product of the problem to be solved, as embodied by the RL environment, and the RL algorithm. While enormous attention has been devoted to RL algorithms, much less is known about the impact of design choices for the RL environment. In this paper, we show that environment design matters in significant ways and document how it can contribute to the brittle nature of many RL results. Specifically, we examine choices related to state representations, initial state distributions, reward structure, control frequency, episode termination procedures, curriculum usage, the action space, and the torque limits. We aim to stimulate discussion around such choices, which in practice strongly impact the success of RL when applied to continuous-action control problems of interest to animation, such as learning to locomote.},
archivePrefix = {arXiv},
arxivId = {2010.04304},
author = {Reda, Daniele and Tao, Tianxin and van de Panne, Michiel},
doi = {10.1145/3424636.3426907},
eprint = {2010.04304},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Reda, Tao, van de Panne - 2020 - Learning to Locomote Understanding How Environment Design Matters for Deep Reinforcement Learning.pdf:pdf},
keywords = {locomotion,reinforcement learning,simulation environments},
title = {{Learning to Locomote: Understanding How Environment Design Matters for Deep Reinforcement Learning}},
year = {2020}
}
@article{Blickhan1993a,
abstract = {Despite impressive variation in leg number, length, position and type of skeleton, similarities of legged, pedestrian locomotion exist in energetics, gait, stride frequency and ground-reaction force. Analysis of data available in the literature showed that a bouncing, spring-mass, monopode model can approximate the energetics and dynamics of trotting, running, and hopping in animals as diverse as cockroaches, quail and kangaroos. From an animal's mechanical-energy fluctuation and ground-reaction force, we calculated the compression of a virtual monopode's leg and its stiffness. Comparison of dimensionless parameters revealed that locomotor dynamics depend on gait and leg number and not on body mass. Relative stiffness per leg was similar for all animals and appears to be a very conservative quantity in the design of legged locomotor systems. Differences in the general dynamics of gait are based largely on the number of legs acting simultaneously to determine the total stiffness of the system. Four- and six-legged trotters had a greater whole body stiffness than two-legged runners operating their systems at about the same relative speed. The greater whole body stiffness in trotters resulted in a smaller compression of the virtual leg and a higher natural frequency and stride frequency. {\textcopyright} 1993 Springer-Verlag.},
author = {Blickhan, R. and Full, R. J.},
doi = {10.1007/BF00197760},
file = {:C$\backslash$:/Users/andre/Documents/Education/Graduate School{\_}ULL/Papers/Similarity{\_}in{\_}multilegged{\_}locomotion{\_}Bouncing{\_}like (1).pdf:pdf},
issn = {03407594},
journal = {Journal of Comparative Physiology A},
keywords = {Energetics,Legs,Locomotion,Mechanics,Stiffness},
number = {5},
pages = {509--517},
title = {{Similarity in multilegged locomotion: Bouncing like a monopode}},
volume = {173},
year = {1993}
}
